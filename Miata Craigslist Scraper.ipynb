{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will gather all US criaglist master URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_scrape_source = 'https://www.craigslist.org/about/sites#US'\n",
    "\n",
    "cities_code = []\n",
    "code_get = requests.get(initial_scrape_source)\n",
    "code_soup = BeautifulSoup(code_get.text, features = 'html.parser')\n",
    "for i in code_soup.find_all('a'):\n",
    "    cities_code.append(i.get('href'))\n",
    "    \n",
    "us_city_links = cities_code[8:424]\n",
    "string_page_codes =['miata', 'mx-5', 'mx5']\n",
    "page_codes = list(range(120, 3000, 120))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are only searching for Miata's, the easiest way is to search for those specific query terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = []\n",
    "links = []\n",
    "combined_link_codes = []\n",
    "\n",
    "for link in us_city_links:\n",
    "    new_list = link + 'search/cto?query='\n",
    "    pages.append(new_list)\n",
    "\n",
    "combined_list = [(p,c) for p in pages for c in string_page_codes]\n",
    "list_df = pd.DataFrame(combined_list)\n",
    "list_df['combined_column'] = list_df[0] + list_df[1]\n",
    "combined_links_codes = list_df['combined_column'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use those combined query URLs to find the individual URL of each Miata posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished First Scan\n"
     ]
    }
   ],
   "source": [
    "url_list = []\n",
    "price = []\n",
    "title = []\n",
    "post_time = []\n",
    "ID = []\n",
    "features = []\n",
    "title2list = []\n",
    "post_text = []\n",
    "\n",
    "\n",
    "for page in combined_links_codes:\n",
    "    listings_p1 = requests.get(page)\n",
    "    soup = BeautifulSoup(listings_p1.text, features = 'html.parser')\n",
    "    listings = soup.find_all('h2')\n",
    "    for i in listings:\n",
    "        try:\n",
    "            link_find = i.find('a', class_ = 'result-title hdrlnk')\n",
    "            link_search = link_find['href']\n",
    "        except TypeError:\n",
    "            link_search = 'null'\n",
    "        url_list.append(link_search)\n",
    "\n",
    "\n",
    "print('Finished First Scan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop any duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(url_list, columns = ['URL'])\n",
    "df.drop_duplicates(subset = ['URL'], inplace = True)\n",
    "miataurl = df['URL'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use list of links to gather posting specific information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Second Scan\n"
     ]
    }
   ],
   "source": [
    "for link in miataurl:\n",
    "    try:\n",
    "        listings_info = requests.get(link)\n",
    "        soup = BeautifulSoup(listings_info.text, features = 'html.parser')\n",
    "    except IndexError: \n",
    "        listing_info = 'null'  \n",
    "    try: \n",
    "        feature_list = soup.find_all('p', class_ = 'attrgroup')[1]\n",
    "        feature_list = feature_list.get_text()\n",
    "    except IndexError:\n",
    "        feature_list = 'null'\n",
    "    features.append(feature_list)\n",
    "    try: \n",
    "        title2 = soup.find_all('p', class_ = 'attrgroup')[0]\n",
    "        title2 = title2.find('span')\n",
    "        title2 = title2.get_text()\n",
    "    except IndexError:\n",
    "        feature_list = 'null'\n",
    "    title2list.append(title2)\n",
    "    try:\n",
    "        posting = soup.find('section', id = 'postingbody')         \n",
    "        posting = posting.get_text()\n",
    "    except AttributeError:\n",
    "        posting = 'null'\n",
    "    post_text.append(posting)\n",
    "    try :\n",
    "        price_search = soup.find('span', class_ = 'price')\n",
    "        price_search = price_search.get_text()\n",
    "    except AttributeError:\n",
    "        price_search = 'null'\n",
    "    price.append(price_search)      \n",
    "    try:\n",
    "        time_find = soup.find('time', class_ = 'date timeago')\n",
    "        time_search = time_find['datetime']\n",
    "    except TypeError:\n",
    "        time_search = 'null'\n",
    "    post_time.append(time_search)\n",
    "    try: \n",
    "        title_list = soup.find('span', id = 'titletextonly')\n",
    "        title_list = title_list.get_text()\n",
    "    except AttributeError:\n",
    "        title_list = 'null'\n",
    "    title.append(title_list)    \n",
    "   \n",
    "print('Finished Second Scan')       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn into a dataframe and start cleaning up by idenifying and removing false positive posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipparams= zip(miataurl, title2list, price, post_time, post_text, features)\n",
    "\n",
    "MiataData = pd.DataFrame(zipparams, columns = (['URL', 'Title', 'Price', 'Post_Time', 'Post_Text', 'Features']))\n",
    "\n",
    "MiataData.drop_duplicates(subset = 'URL', keep = 'first', inplace = True)\n",
    "\n",
    "def find_miata(x):\n",
    "    if 'miata' in x:\n",
    "        return 1\n",
    "    elif 'mx-5' in x:\n",
    "        return 1\n",
    "    elif 'mx5' in x:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "MiataData['Is_Miata_Title'] = MiataData.Title.apply(lambda x: find_miata(x.lower()))\n",
    "MiataData['Is_Miata_URL'] = MiataData.URL.apply(lambda x: find_miata(x.lower()))\n",
    "MiataData['Sum_Test'] = MiataData['Is_Miata_Title'] + MiataData['Is_Miata_URL']\n",
    "\n",
    "def add_two(x):\n",
    "    if x >= 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "MiataData['Is_Miata'] = MiataData['Sum_Test'].apply(lambda x: add_two(x))\n",
    "MiataData = MiataData[MiataData['Is_Miata'] == True]\n",
    "MiataData = MiataData[['URL', 'Title', 'Price', 'Post_Time', 'Post_Text', 'Features']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's get the year from our Titles we extracted.  I'll start with the first Title and then try the second if it is not provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MiataData['Year'] = MiataData.Title.str.extract('(^\\d*)')\n",
    "MiataData['Year'].replace('', np.nan, inplace=True)\n",
    "\n",
    "\n",
    "MiataData.reset_index(inplace=True)\n",
    "\n",
    "MiataData = MiataData.dropna(subset = ['Year'])\n",
    "MiataData['Year']  = MiataData['Year'].astype(int)\n",
    "\n",
    "def year_format(x):\n",
    "    if 0 <= x <= 19:\n",
    "        return '200' + str(x)\n",
    "    elif 20 <= x <= 99:\n",
    "        return '19' + str(x)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "MiataData['Year'] = MiataData.Year.apply(lambda x: year_format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Year is taken care of let's find out where these Miatas are located. \n",
    "I'm going to pull the data from the URL string and join it to a spreadsheet I found of all craigslist URL codes associated to their locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_lookup = pd.read_excel('/Users/dcramer451/Desktop/Script/citydecoder.xlsx')  \n",
    "URL_split = MiataData['URL'].str.split('/', expand = True)\n",
    "join_back = MiataData.merge(URL_split, how = 'left', left_index = True, right_index = True)\n",
    "MiataData = join_back.merge(city_lookup, how = 'left', right_on = 'URL_string', left_on = 2)\n",
    "\n",
    "MiataData = MiataData[[ 'URL', 'Title','Price','Post_Time', 'Post_Text', 'Features','Year', 'Location', 'State']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the time from the date next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = MiataData['Post_Time'].str.split(\" \", expand = True)\n",
    "date_time = date_time.apply(lambda x: x.str.strip())\n",
    "df = df.merge(date_time, left_index = True, right_index = True, how = 'left')       \n",
    "df = df.rename(columns= {0 : 'Posted_date', 1 : 'Posted_time'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes the tricky part.  Currently, the features are clumped together in one cell with a bunch of newlines. \n",
    "They need to be separated and cleaned before this can happen. \n",
    "To do this, I'm going to write a series of formulas that will split each into it's own column.\n",
    "Once I'm done cleaning, I'll rejoin this data back to the MiataData dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fb467ece47be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m#Only odd columns have values so I select those, I also convert Nones to NaNs since they are easier to work with.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mfeatures_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mfeatures_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_split\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features_split' is not defined"
     ]
    }
   ],
   "source": [
    "#Here are the conditions I'll use the locate and label each feature. \n",
    "\n",
    "def find_condition(x):\n",
    "    if 'condition' in x:\n",
    "        return x\n",
    "    else:\n",
    "        return 'NA'\n",
    "\n",
    "def find_fuel(x):\n",
    "    if 'fuel' in x:\n",
    "        return x\n",
    "    else:\n",
    "        return 'NA'\n",
    "    \n",
    "def find_cylinders(x):\n",
    "    if 'cylinders' in x:\n",
    "      return x\n",
    "    else:\n",
    "        return 'NA'\n",
    "\n",
    "def find_title(x):\n",
    "    if 'title' in x:\n",
    "        return x\n",
    "    else:\n",
    "        return 'NA'    \n",
    "\n",
    "def find_drive(x):\n",
    "    if 'drive' in x:\n",
    "        return x\n",
    "    else:\n",
    "        return 'NA'   \n",
    " \n",
    "def find_transmission(x):\n",
    "    if 'transmission' in x:\n",
    "        return x\n",
    "    else:\n",
    "        return 'NA'\n",
    "    \n",
    "def find_odometer(x):\n",
    "    if 'odometer' in x:\n",
    "        return x\n",
    "    else:\n",
    "        return 'NA'\n",
    "\n",
    "def find_color(x):\n",
    "    if 'color' in x:\n",
    "        return x\n",
    "    else:\n",
    "        return 'NA'\n",
    "\n",
    "def find_size(x):\n",
    "    if 'size' in x:\n",
    "        return x\n",
    "    else:\n",
    "        return 'NA' \n",
    " \n",
    "def find_type(x):\n",
    "    if 'type' in x:\n",
    "        return x\n",
    "    else:\n",
    "        return 'NA'\n",
    "\n",
    "def replace_none(x):\n",
    "    if x is None:\n",
    "        return '0' \n",
    "    else:\n",
    "        return x      \n",
    "    \n",
    "#Only odd columns have values so I select those, I also convert Nones to NaNs since they are easier to work with.\n",
    "\n",
    "features_split = features_split[[1,3,5,7,9,11,13,15,17,19,21]]\n",
    "\n",
    "features_split = features_split.replace('', np.nan)\n",
    "\n",
    "features_split = features_split.applymap(str)\n",
    "    \n",
    "split_list = [1,3,5,7,9,11,13,15,17,19,21]\n",
    "\n",
    "condition_columns = []\n",
    "cylinder_columns = []\n",
    "fuel_columns = []\n",
    "transmission_columns = []\n",
    "title_columns = []\n",
    "odometer_columns = []\n",
    "paint_columns = []\n",
    "size_columns = []\n",
    "type_columns = []\n",
    "\n",
    "#This itterates through each column and finds when a certain feature is listed using the above conditional formulas.\n",
    "for i in split_list:\n",
    "    features_split['condition' + str(i)] = features_split[i].apply(lambda x: find_condition(x))\n",
    "    features_split['cylinders' + str(i)] = features_split[i].apply(lambda x: find_cylinders(x))\n",
    "    features_split['fuel' + str(i)] = features_split[i].apply(lambda x: find_drive(x))\n",
    "    features_split['transmission' + str(i)] = features_split[i].apply(lambda x: find_transmission(x))\n",
    "    features_split['title' + str(i)] = features_split[i].apply(lambda x: find_title(x))\n",
    "    features_split['odometer' + str(i)] = features_split[i].apply(lambda x: find_odometer(x))\n",
    "    features_split['paint' + str(i)] = features_split[i].apply(lambda x: find_color(x))\n",
    "    features_split['size' + str(i)] = features_split[i].apply(lambda x: find_size(x))\n",
    "    features_split['type' + str(i)] = features_split[i].apply(lambda x: find_type(x))\n",
    "\n",
    "\n",
    "#the columns are built as lists using the values pulled from the formulas and loop. \n",
    "for i in split_list:\n",
    "    condition_columns.append('condition' + str(i))\n",
    "    cylinder_columns.append('cylinders' + str(i))\n",
    "    fuel_columns.append('fuel' + str(i))\n",
    "    transmission_columns.append('transmission' + str(i))\n",
    "    title_columns.append('title' + str(i))\n",
    "    odometer_columns.append('odometer' + str(i))\n",
    "    paint_columns.append('paint' + str(i))\n",
    "    size_columns.append('size' + str(i))\n",
    "    type_columns.append('type' + str(i))\n",
    "\n",
    "\n",
    "#Additional cleanup\n",
    "features_split = features_split.replace('NA',\"\")\n",
    "\n",
    "condition = features_split[condition_columns]\n",
    "cylinders = features_split[cylinder_columns]\n",
    "fuel = features_split[fuel_columns]\n",
    "transmission = features_split[transmission_columns]\n",
    "title_status = features_split[title_columns]\n",
    "odometer = features_split[odometer_columns]\n",
    "paint = features_split[paint_columns]\n",
    "size = features_split[size_columns]\n",
    "vtype = features_split[type_columns]\n",
    "\n",
    "condition['Condition'] = condition.sum(axis=1).astype(str)\n",
    "cylinders['Cylinders'] = cylinders.sum(axis=1).astype(str)\n",
    "fuel['Fuel'] = fuel.sum(axis=1).astype(str)\n",
    "transmission['Transmission'] = transmission.sum(axis=1).astype(str)\n",
    "title_status['Title_status'] = title_status.sum(axis=1).astype(str)\n",
    "odometer['Odometer'] = odometer.sum(axis=1).astype(str)\n",
    "paint['Paint'] = paint.sum(axis=1).astype(str)\n",
    "size['Size'] = size.sum(axis=1).astype(str)\n",
    "vtype['Type'] = vtype.sum(axis=1).astype(str)\n",
    "\n",
    "condition = condition[['Condition']]\n",
    "cylinders = cylinders[['Cylinders']]\n",
    "fuel = fuel[['Fuel']]\n",
    "transmission = transmission[['Transmission']]\n",
    "title_status = title_status[['Title_status']]\n",
    "odometer = odometer[['Odometer']]\n",
    "paint = paint[['Paint']]\n",
    "size = size[['Size']]\n",
    "vtype = vtype[['Type']]\n",
    "\n",
    "#Split values based on common \":\" delimiter. \n",
    "\n",
    "condition = condition.Condition.str.split(\":\", expand = True)\n",
    "cylinders = cylinders.Cylinders.str.split(\":\", expand = True)\n",
    "fuel = fuel.Fuel.str.split(\":\", expand = True)\n",
    "transmission = transmission.Transmission.str.split(\":\", expand = True)\n",
    "title_status = title_status.Title_status.str.split(\":\", expand = True)\n",
    "odometer = odometer.Odometer.str.split(\":\", expand = True)\n",
    "paint = paint.Paint.str.split(\":\", expand = True)\n",
    "size = size.Size.str.split(\":\", expand = True)\n",
    "vtype = vtype.Type.str.split(\":\", expand = True)\n",
    "\n",
    "condition = condition[[1]]\n",
    "cylinders = cylinders[[1]]\n",
    "fuel = fuel[[1]]\n",
    "transmission = transmission[[1]]\n",
    "title_status = title_status[[1]]\n",
    "odometer = odometer[[1]]\n",
    "paint = paint[[1]]\n",
    "size = size[[1]]\n",
    "vtype = vtype[[1]]\n",
    "\n",
    "\n",
    "#Join them all together. \n",
    "\n",
    "features_merge = condition.merge(cylinders, how = 'left', left_index = True, right_index = True)\n",
    "features_merge = features_merge.merge(fuel, how = 'left', left_index = True, right_index = True)\n",
    "features_merge = features_merge.merge(transmission, how = 'left', left_index = True, right_index = True)\n",
    "features_merge = features_merge.merge(title_status, how = 'left', left_index = True, right_index = True)\n",
    "features_merge = features_merge.merge(odometer, how = 'left', left_index = True, right_index = True)\n",
    "features_merge = features_merge.merge(paint, how = 'left', left_index = True, right_index = True)\n",
    "features_merge = features_merge.merge(size, how = 'left', left_index = True, right_index = True)\n",
    "features_merge = features_merge.merge(vtype, how = 'left', left_index = True, right_index = True)\n",
    "\n",
    "features_merge.columns = ['Condition', 'Cylinders', 'Fuel', 'Transmission', 'Title_status', 'Odometer', 'Paint', 'Size', 'Vehical_type']\n",
    "\n",
    "#Additional cleanup    \n",
    "\n",
    "features_merge = features_merge.apply(lambda x: x.str.strip())\n",
    "features_merge = features_merge.apply(lambda x : x.str.title())  \n",
    "\n",
    "features_merge['Odometer'] = features_merge['Odometer'].apply(lambda x: replace_none(x))\n",
    "features_merge['Odometer'] = features_merge['Odometer'].astype(int)\n",
    "\n",
    "features_merge['Cylinders'] = features_merge.Cylinders.str.extract('(^\\d*)')\n",
    "\n",
    "\n",
    "#Join back to main dataset.\n",
    "\n",
    "df = MiataData.merge(features_merge, how = 'left', left_index = True, right_index = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, a lot of these features are unneccesary and can be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['URL', 'Title', 'Price', 'Post_Time', 'Post_Text', 'Year','Location', 'State', 'Condition', 'Transmission', 'Title_status', 'Odometer']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's tackle the posting body.\n",
    "Embedded in there are some newlines and a ubiquitous QR Code Link to This Post. Both must be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Post_Text'] = df.Post_Text.replace('\\n',\"\", regex = True)\n",
    "df['Post_Text'] = df.Post_Text.replace('QR Code Link to This Post', \"\", regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the posting body by itself, we should try to see if we can flush any data out that might be missing from the features. \n",
    "One thing that comes to mind is the odometer reading.  Using some regex and conditions, we should be able to fill in some missing odometers with this information. \n",
    "\n",
    "Let's first try just extracting some numbers and then filtering them so they don't match model years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posting_extract =  df.Post_Text.str.extract('(\\d\\d\\d\\d\\d)', expand = True)\n",
    "posting_extract['2'] = df.Post_Text.str.extract('(\\d\\d\\d\\d\\d\\d)', expand = True)\n",
    "posting_extract['3'] = df.Post_Text.str.extract('(\\d\\d\\d\\d)', expand = True)\n",
    "\n",
    "\n",
    "def find_k(x):\n",
    "    return re.findall('(\\d+)k(i?)', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That got us some results, but not all of the possibilities.  Many owners list this milage as \\d\\d\\dk.  Let's use regex to find those as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posting_extract['4'] = df.Post_Text.apply(find_k)\n",
    "posting_extract['4'] = posting_extract['4'].str[0]\n",
    "posting_extract['4'] = posting_extract['4'].str[0]\n",
    "posting_extract['4'] = posting_extract['4'].fillna(0) \n",
    "posting_extract['4'] = posting_extract['4'].astype(int)\n",
    "\n",
    "def kilo_filter(x):\n",
    "    if x < 999:\n",
    "        return x * 1000\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def kilo_filter2(x):\n",
    "    if x < 999:\n",
    "        return x * 1000\n",
    "    else:\n",
    "        return x    \n",
    "\n",
    "posting_extract['4'] = posting_extract['4'].apply(lambda x: kilo_filter(x))    \n",
    "posting_extract['3'] = posting_extract['3'].fillna(0)\n",
    "posting_extract['3'] = posting_extract['3'].astype(int)\n",
    "\n",
    "def extract_condition(x):\n",
    "    if x > 2030:\n",
    "        return x\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "posting_extract['3'] = posting_extract['3'].apply(lambda x: extract_condition(x))\n",
    "posting_extract['2'] = posting_extract['2'].fillna(posting_extract[0])\n",
    "posting_extract['2'] = posting_extract['2'].fillna(posting_extract['3'])\n",
    "posting_extract['2'] = posting_extract['2'].replace(0, posting_extract['4'])\n",
    "posting_extract = posting_extract[['2']]\n",
    "\n",
    "#Now merge them back together. \n",
    "\n",
    "df = df.merge(posting_extract, how = 'left', left_index = True, right_index = True)\n",
    "\n",
    "df['Odometer'] = df['Odometer'].replace(0, df['2'])\n",
    "\n",
    "df = df[['URL', 'Title', 'Price', 'Post_Time', 'Post_Text', 'Year', 'Location','State', 'Condition', 'Transmission', 'Title_status', 'Odometer']]\n",
    "\n",
    "df['Odometer'] = df['Odometer'].astype(int)\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "df['Odometer'] = df['Odometer'].apply(lambda x: kilo_filter2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load prep-data into SQLite database to build initial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('MiataDatabase.db')   \n",
    "c = conn.cursor() \n",
    "df.to_sql('Prep_Data', conn, if_exists= 'append') \n",
    "c.execute(''' SELECT DISTINCT * from Prep_Data''')\n",
    "df = pd.DataFrame(c.fetchall())\n",
    "df.columns = ['Index' ,'URL', 'Title', 'Price', 'Post_Time', 'Post_Text', 'Year', 'Location','State', 'Condition', 'Transmission', 'Title_status', 'Odometer']\n",
    "c.close()\n",
    "\n",
    "df.reset_index(inplace = True)\n",
    "\n",
    "df = df[['URL', 'Title', 'Price', 'Post_Time', 'Post_Text', 'Year', 'Location','State', 'Condition', 'Transmission', 'Title_status', 'Odometer']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up Price & Year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Price'] != 'null']\n",
    "\n",
    "df['Price'] = df['Price'].str.replace('$', \"\")\n",
    "df['Price'] = df['Price'].str.replace(',', \"\")\n",
    "df['Price'] = df['Price'].astype(int)\n",
    "\n",
    "df = df[df['Price'] > 1000]\n",
    "df = df[df['Price'] < 100000]\n",
    "\n",
    "df = df[df['Year'] > 1989]\n",
    "df = df[df['Year'] < 2022]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have some 0's in the odometer columns that need to be filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odometer_impute_condition(x):\n",
    "    if x == 0:\n",
    "        return \"Estimated Mileage\"\n",
    "    else:\n",
    "        return \"Actual Mileage\"\n",
    "\n",
    "df['Odometer_Imputation'] = df['Odometer'].apply(lambda x: odometer_impute_condition(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since year and mileage are usually related, I'm going to find the mean for all years and use that to estimate the mileage of the vehicals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odo_average = df[df['Odometer'] != 0]\n",
    "odo_average = odo_average[['Year', 'Odometer']]\n",
    "odo_average = odo_average.groupby('Year').mean()\n",
    "odo_average.reset_index(inplace = True)\n",
    "odo_average['Odometer'] = odo_average['Odometer'].astype(int)\n",
    "\n",
    "df = df.merge(odo_average, how = 'left', left_on = 'Year', right_on = 'Year')\n",
    "df['Odometer'] = df['Odometer_x'].replace(0, df['Odometer_y'])\n",
    "\n",
    "#Let's break out the date and see if we can use it for something interesting later. \n",
    "df = df[df['Post_Time'] != 'null']\n",
    "df['Post_Time'] = df['Post_Time'].str.split('T', expand = True)\n",
    "df['Post_Time'] = df['Post_Time'].str[:10]\n",
    "df['Day'] = pd.to_datetime(df['Post_Time'])\n",
    "\n",
    "df['Day'] = df['Day'].dt.day_name()\n",
    "df['Month'] = pd.to_datetime(df['Post_Time'])\n",
    "df['Month'] = df['Month'].dt.month_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other is almost always used when people confuse paddle shifters with manual.  This is a safe assumption. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_transmission(x):\n",
    "    if x == 'Other':\n",
    "        return \"Automatic\"\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "df['Transmission'] = df['Transmission'].apply(lambda x: fix_transmission(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define generation codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generations(x):\n",
    "    if 1998 >= x >= 1989:\n",
    "        return 'NA'\n",
    "    elif 2004 >= x >= 1999:\n",
    "        return 'NB'\n",
    "    elif 2015 >= x >= 2005:\n",
    "        return 'NC'\n",
    "    elif 2021 >= x >= 2016:\n",
    "        return 'ND'\n",
    "    \n",
    "df['Generation'] = df['Year'].apply(lambda x: generations(x))\n",
    "df = df[['URL', 'Title', 'Price', 'Post_Time', 'Post_Text', 'Year', 'Location','State', 'Condition', 'Transmission', 'Title_status', 'Odometer_Imputation', 'Odometer', 'Day', 'Month','Generation']]\n",
    "df.columns = ['URL', 'Title', 'Price', 'Post_Time', 'Post_Text', 'Year', 'Location','State', 'Condition', 'Transmission', 'Title_Status', 'Odometer_Imputation', 'Odometer', 'Day', 'Month','Generation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fill the missing condition let's have some fun and use a basic natural language processing pipeline to fill in the blanks use the posting body text. I played around with this for awhile and got the best results using SHDClassifier.  This is a fairly common machining learning processed used.  Alternatively, a multinomial naive bayes (MultinomialNB) will work as well. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_standardize(x):\n",
    "    if x == 'Salvage':\n",
    "        return 'Fair'\n",
    "    elif x == 'Like New':\n",
    "        return 'Excellent'\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def estimated_condition(x):\n",
    "    if pd.isna(x):\n",
    "        return \"Estimated\"\n",
    "    else:\n",
    "        return \"Actual\"\n",
    "\n",
    "df['Condition'] = df['Condition'].apply(lambda x: condition_standardize(x))\n",
    "df['Condition_Estimated'] = df[\"Condition\"].apply(lambda x: estimated_condition(x))\n",
    "df = df[df['Condition'] != 'New']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = df.drop_duplicates(subset = 'URL')\n",
    "\n",
    "condition_exists = df[df.Condition.notnull()]\n",
    "condition_predict = df[df.Condition.isnull()]\n",
    "condition_predict.reset_index(inplace = True)\n",
    "predict_set = condition_predict['Post_Text']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.lower()\n",
    "    text = REPLACE_BY_SPACE_RE.sub('', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text\n",
    "\n",
    "condition_exists['clean_post'] = condition_exists['Post_Text'].apply(clean_text)\n",
    "\n",
    "X = condition_exists.clean_post\n",
    "Y = condition_exists.Condition\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state = 42)\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', SGDClassifier(loss = 'hinge',random_state = 42)),])\n",
    "\n",
    "nb.fit(X_train, y_train)    \n",
    "\n",
    "y_pred = nb.predict(predict_set)\n",
    "\n",
    "\n",
    "predicted_condition = pd.DataFrame(y_pred, columns = ['cond_pred'])\n",
    "condition_predict = condition_predict.merge(predicted_condition, left_index = True, right_index = True)\n",
    "condition_predict = condition_predict.reset_index()\n",
    "condition_predict = condition_predict[['URL', 'Title', 'Price', 'Post_Time', 'Post_Text', 'Year', 'Location','State', 'Condition', 'Transmission', 'Title_Status', 'Odometer_Imputation', 'Odometer', 'Day', 'Month', 'Generation', 'Condition_Estimated', 'cond_pred']]\n",
    "\n",
    "\n",
    "condition_predict['Condition'] = condition_predict['cond_pred']\n",
    "\n",
    "\n",
    "\n",
    "condition_predict = condition_predict[['URL', 'Title', 'Price', 'Post_Time', 'Post_Text', 'Year', 'Location','State', 'Condition', 'Transmission', 'Title_Status', 'Odometer_Imputation', 'Odometer', 'Day', 'Month', 'Generation', 'Condition_Estimated']]\n",
    "\n",
    "df = pd.concat([condition_predict, condition_exists])\n",
    "\n",
    "df = df[['URL', 'Title', 'Price', 'Post_Time', 'Post_Text', 'Year', 'Location','State', 'Condition', 'Transmission', 'Title_Status', 'Odometer_Imputation', 'Odometer', 'Day', 'Month', 'Generation', 'Condition_Estimated']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find if hardtops are included in the text body. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hardtop(x):\n",
    "    if 'hardtop' in x:\n",
    "        return 'Yes'\n",
    "    elif 'Hardtop' in x:\n",
    "        return 'Yes'\n",
    "    elif 'RF' in x:\n",
    "        return 'Yes'\n",
    "    elif 'rf' in x:\n",
    "        return 'Yes'\n",
    "    elif 'PHRT' in x:\n",
    "        return 'Yes'\n",
    "    elif 'phrt' in x:\n",
    "        return x\n",
    "    elif 'hard top' in x:\n",
    "        return 'Yes'\n",
    "    elif 'Hard top' in x:\n",
    "        return 'Yes'\n",
    "    elif 'Hard Top' in x:\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'\n",
    "\n",
    "df['Hardtop_Desc'] = df['Post_Text'].apply(lambda x: hardtop(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backup to secondary prepared data table, export, and then check if previous links are currenly active.  Label if necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('MiataDatabase.db')   \n",
    "c = conn.cursor() \n",
    "\n",
    "df.to_sql('Finished_Data', conn, if_exists= 'append', index = False) \n",
    "c.execute(''' SELECT DISTINCT * from Finished_Data''')\n",
    "\n",
    "MiataDatabase = pd.DataFrame(c.fetchall())\n",
    "\n",
    "c.close()\n",
    "\n",
    "MiataDatabase.columns = ['URL', 'Title', 'Price', 'Post_Time', 'Post_Text', 'Year', 'Location', 'State', 'Condition', 'Transmission', 'Title_Status', 'Odometer_Imputation', 'Odometer', 'Day', 'Month', 'Generation', 'Condition_Estimated', 'Hardtop_Desc']\n",
    "\n",
    "MiataDatabase = MiataDatabase.drop_duplicates(subset = 'URL')\n",
    "#Now that our data is stored and able to be built over time we need to check if the URLs coming out of the database are currently active or if they have expired.\n",
    "\n",
    "\n",
    "url_check = MiataDatabase.URL.tolist()\n",
    "code_list = []\n",
    "for i in url_check:\n",
    "    response = requests.get(i)\n",
    "    code_list.append(response.status_code)\n",
    "\n",
    "def url_code(x):\n",
    "    if x == 200:\n",
    "        return \"Active Link\"\n",
    "    elif x == 404:\n",
    "        return \"Inactive Link\"\n",
    "\n",
    "code_list = pd.DataFrame(code_list, columns = ['Active_Links'])\n",
    "\n",
    "MiataDatabase = MiataDatabase.merge(code_list, how = 'left', left_index = True, right_index = True)\n",
    "\n",
    "\n",
    "MiataDatabase['Active_Links'] = MiataDatabase['Active_Links'].apply(lambda x: url_code(x))\n",
    "\n",
    "\n",
    "MiataDatabase.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use random forest regressor to predict pricing.  Currently, the model does not have enough data to provide accurate recommendations but as the dataset grows, I believe it will become more reliable. \n",
    "\n",
    "After that, export to excel for use in dashboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MiataDatabaseAnalysis = MiataDatabase[['Price','Year', 'Odometer','Condition', 'Hardtop_Desc', 'Title_Status', 'State', 'Generation']]\n",
    "dummies = pd.get_dummies(MiataDatabaseAnalysis[['Condition', 'Hardtop_Desc', 'Title_Status', 'State', 'Generation']])\n",
    "\n",
    "MiataDatabaseAnalysis = MiataDatabaseAnalysis.merge(dummies, left_index = True, right_index = True, how = 'left')\n",
    "\n",
    "column_list = dummies.columns\n",
    "column_list = column_list.tolist()\n",
    "column_list.extend(('Year' , 'Odometer'))\n",
    "\n",
    "\n",
    "Y = MiataDatabaseAnalysis['Price']\n",
    "X = MiataDatabaseAnalysis[column_list]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state = 42)\n",
    "\n",
    "reg = RandomForestRegressor(n_estimators = 60, random_state = 42)\n",
    "reg.fit(X_train, y_train)\n",
    "print('Current Number of Listings: ' + str(len(Y)))\n",
    "print('Model currently scoring at ' + str((reg.score(X_test, y_test))))\n",
    "\n",
    "predicted_values = reg.predict(X)   \n",
    "\n",
    "pred_values = pd.DataFrame(predicted_values, columns = ['Predicted_Price'])\n",
    "    \n",
    "df = MiataDatabase.merge(pred_values, left_index = True, right_index = True, how = 'left')\n",
    "   \n",
    "df.to_excel('outputfordashboard.xlsx') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
